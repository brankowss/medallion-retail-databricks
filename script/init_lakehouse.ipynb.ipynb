{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e77fa324-9368-486d-abef-24924b16b866",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Lakehouse Initialization\n",
    "\n",
    "This notebook initializes the Databricks Lakehouse:\n",
    "- Uses `workspace` catalog\n",
    "- Creates bronze, silver, and gold schemas\n",
    "- Creates a volume for raw CSV source files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ddfb96dc-b27b-4279-9362-daa862977c82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Select Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf779305-6fab-40d9-8d84-24d7f58d66bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG workspace;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eec85bae-24f9-420c-ac40-83e9a4a8b348",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create Lakehouse Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "660bbafb-8548-470f-85fc-dd1f183c15d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE SCHEMA IF NOT EXISTS bronze\n",
    "COMMENT 'Bronze layer: raw ingested data';\n",
    "\n",
    "CREATE SCHEMA IF NOT EXISTS silver\n",
    "COMMENT 'Silver layer: cleaned and transformed data';\n",
    "\n",
    "CREATE SCHEMA IF NOT EXISTS gold\n",
    "COMMENT 'Gold layer: business-ready data';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4febf731-276c-425b-b43a-4699adf718a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c2171d2-a2f9-4236-b9f6-588e6ec805ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE VOLUME IF NOT EXISTS workspace.bronze.raw_sources\n",
    "COMMENT 'Volume for raw source files (CSV)';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0728106-625c-445d-be80-accd43dc80b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Move Data from GitHub to Volume\n",
    "Since this project uses **Unity Catalog Volumes** for data ingestion, we need to copy the CSV files from the cloned GitHub repository into the designated Volume. \n",
    "\n",
    "This script automates the process so the project works \"out of the box\" for anyone who clones it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db4e500e-9a10-45cf-80c4-387f854781b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def find_datasets_path():\n",
    "    \"\"\"\n",
    "    Dynamically locates the 'datasets' folder within the cloned repository.\n",
    "    \"\"\"\n",
    "    curr = os.getcwd()\n",
    "    while curr != \"/\":\n",
    "        possible_path = os.path.join(curr, \"datasets\")\n",
    "        if os.path.exists(possible_path):\n",
    "            return possible_path\n",
    "        curr = os.path.dirname(curr)\n",
    "    return None\n",
    "\n",
    "# Identify source and destination\n",
    "datasets_local_path = find_datasets_path()\n",
    "volume_base = \"/Volumes/workspace/bronze/raw_sources\"\n",
    "\n",
    "if datasets_local_path:\n",
    "    try:\n",
    "        # Ensure destination sub-directories exist in the Volume\n",
    "        os.makedirs(f\"{volume_base}/source_crm\", exist_ok=True)\n",
    "        os.makedirs(f\"{volume_base}/source_erp\", exist_ok=True)\n",
    "\n",
    "        # Iterate through folders and copy CSV files\n",
    "        for folder in [\"source_crm\", \"source_erp\"]:\n",
    "            src_dir = os.path.join(datasets_local_path, folder)\n",
    "            dest_dir = os.path.join(volume_base, folder)\n",
    "            \n",
    "            print(f\"Syncing folder: {folder}...\")\n",
    "            for file_name in os.listdir(src_dir):\n",
    "                if file_name.endswith(\".csv\"):\n",
    "                    src_file = os.path.join(src_dir, file_name)\n",
    "                    dest_file = os.path.join(dest_dir, file_name)\n",
    "                    \n",
    "                    # Using shutil.copy to bypass WorkspaceLocalFileSystem restrictions\n",
    "                    shutil.copy(src_file, dest_file)\n",
    "                    print(f\"  ✅ Copied: {file_name}\")\n",
    "        \n",
    "        print(f\"\\n Success! All datasets have been migrated to: {volume_base}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during data seeding: {e}\")\n",
    "else:\n",
    "    print(\"❌ Critical Error: 'datasets' folder not found in the repository path!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8553090258546578,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "init_lakehouse.ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
